'''
file:           /curriculum//encoder.py
author:         Michal Glos (xglosm01)
established:    20.1.2023
last modified:  21.1.2023

                        ##################
                        #@$%&        &%$@#
                        #!   <(o )___   !#
                        #!    ( ._> /   !#
                        #!     `---'    !#
                        #@$%&        &%$@#
                        ##################

This file defines Encoder class, which would be used to process input observations
'''

import numpy as np

class MultiAgentMultiModelReplayBuffer:
    '''
    This buffer would implement shared space for all models used for training
    
    Entries would be separated though - each antry would carry a flag if was generated from primary or secondary model\
    '''
    def __init__(self, memory, obs_space, actions_count, agents_count):
        '''
        Initialize the MultiAgentMultiModelReplayBuffer
        
        @params:
            memory:             Size of replay buffer arrays
            obs_space:          Shape of agents observation
            actions_count:      Number of possible actions of a single agent
            agent_count:        Number of agents
            batch_size
        '''

        # Save the config
        self.max_memory = memory
        self.overflowing_pointer = 0
        self.agents_count = agents_count
        self.a_inputs_size = obs_space
        self.c_inputs_size = (obs_space + actions_count) * agents_count
        self.global_state_size = obs_space * agents_count
        self.n_actions = actions_count

        # Initialize common memory
        self.memory = {
            # primary - whether data were generated by primary model or not
            'primary': np.zeros((self.max_memory), dtype=bool),
            'Q': np.zeros((self.max_memory)),
            'global_states': np.zeros((self.max_memory, self.global_state_size)),
            'global_next_states': np.zeros((self.max_memory, self.global_state_size)),
            'reward': np.zeros((self.max_memory, self.agents_count)),
            'done': np.zeros((self.max_memory), dtype=bool),
            # Initialize per-actor memory. Since all actors are identical in terms of observation and action space,
            # store them in a numpy array. Second dimensions are therefore always the dimensions of agents 
            'actor_states': np.zeros((self.max_memory, self.agents_count, self.a_inputs_size)),
            'actor_next_states': np.zeros((self.max_memory, self.agents_count, self.a_inputs_size)),
            'actions': np.zeros((self.max_memory, self.agents_count, self.n_actions))
        }


    def log_new_step(self, g_state, g_next_state, actions, reward, obs, next_obs, done, q_value, primary=True):
        '''
        Log single tranisition from interacting with the environment
        
        @params:
            raw_obs:            Observation of each agent
            global_next_states: State to be interacted with
            global_next_states: State to obtain after performing actions in env in state
            actions:            Actions performed by agents
            reward:             Reward obtained
            obs:                List of agent observations
            next_obs:           Following observations
            done:               Is this the final step in the environment?
            primary:            Are data obtained from primary neural network?
            q_value:            The approximated Q-value of critic
        '''
        # Obtain the relative pointer
        memory_pointer = self.overflowing_pointer % self.max_memory
        # Save the data
        self.memory['primary'][memory_pointer] = primary
        self.memory['Q'][memory_pointer] = q_value
        self.memory['global_states'][memory_pointer] = g_state
        self.memory['global_next_states'][memory_pointer] = g_next_state
        self.memory['reward'][memory_pointer][:] = reward
        self.memory['done'][memory_pointer] = done
        self.memory['actor_states'][memory_pointer] = obs
        self.memory['actor_next_states'][memory_pointer] = next_obs
        self.memory['actions'][memory_pointer] = actions

        # Update the pointer value
        self.overflowing_pointer += 1

    def slice_data_indexes(self, primary=None):
        '''
        Slice data with indexes - provide indexes for required data to self.memory arrays
        
        @params:
            primary:    Requrie data only from primary (True), secondary (False) or whichever (None) model 
        '''
        true_size = min(self.overflowing_pointer, self.max_memory)
        if primary is None:
            return np.arange(true_size)
        else:
            return np.where((self.memory['primary'][:true_size] == primary))[0]

    def sample_buffer(self, batch, with_return=False, primary=None):
        '''
        Obtain a sample of buffer
        
        @params:
            batch:          Batch size (How many samples to return)
            with_return:    Sample a set of possible repeating elements
            primary:        Require data from primary (primary = True), secondary (primary = False) or both (primary = None)
        '''
        # Check if we do even have enough data to sample from
        #   Computing for nor extra requirements is simple, when exact model is requested, count entries from that model only
        ids = self.slice_data_indexes(primary=primary)
        assert len(ids) >= batch or with_return, f'Too little data in replay buffer to sample pimary: {primary} data, only {len(ids)} found.'

        # Now let's choose the saple indexes, here is the place to potentially apply some heuristics TODO!
        batch_ids = np.random.choice(ids, batch, replace=with_return)

        return {key: val[batch_ids] for key, val in self.memory.items()}
