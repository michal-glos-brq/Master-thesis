'''
file:           /agent/models/regular.py
author:         Michal Glos (xglosm01)
established:    20.1.2023
last modified:  20.1.2023

                        ##################
                        #@$%&        &%$@#
                        #!   <(o )___   !#
                        #!    ( ._> /   !#
                        #!     `---'    !#
                        #@$%&        &%$@#
                        ##################

This file defines the RegularNN (regular neural network) for policy estimation
'''
import sys
import torch

from agent.models.encoder import Encoder

class RegularNN(torch.nn.Module):
    '''
    This is regular linear neural network providing actions (pi(s) = a) given observation
    '''
    def __init__(self, env, obs_raw, lr, device,
                layers_enc, layer_opts_enc, act_enc, out_act_enc, out_act_opts_enc, 
                layers_policy, layer_opts_policy, act_policy, out_act_policy, out_act_opts_policy,
                **kwargs):
        '''
        Initialize RegularNN. Comprises of 2 encoder blocks (as Critic class from /agenst/models/critic.py file)

        @params:
            env:                    Env object instance (wrapper)
            lr:                     Learning rate
            obs_raw:                Is observation RAW IMAGE or extracted information vector?
            layers_enc:             Encoder parameter `layers`
            layer_opts_enc:         Encoder parameter `layer_opts`
            act_enc:                Encoder parameter `act`
            out_act_enc:            Encoder parameter `out_act`
            out_act_opts_enc:       Encoder parameter `out_act_opts`
            layers_policy:          Policy neural network (Encoder class) parameter `layers`
            layer_opts_policy:      Policy neural network (Encoder class) parameter `layer_opts`
            act_policy:             Policy neural network (Encoder class) parameter `act`
            out_act_policy:         Policy neural network (Encoder class) parameter `out_act`
            out_act_opts_policy:    Policy neural network (Encoder class) parameter `out_act_opts`
        '''
        # Initialize parent class
        super(RegularNN, self).__init__()

        # First check neural network shape
        input_size = env.observation_space
        action_size = env.action_space 
        if layers_enc[0][0] != input_size:
            print(f'Warning: The dimension of first layer of critic was in: {layers_enc[0][0]};' + 
                  f'out: {layers_enc[0][1]}. Changing it to in: {input_size}; now{layers_enc[0][1]}.', file=sys.stderr)
            layers_enc[0] = (input_size, layers_enc[0][1])
            
        if layers_enc[-1][1] != layers_policy[0][0]:
            print(f'Error: The dimension of last layer of encoder part of critic was: {layers_enc[-1][1]};' + 
                  f' the policy network is: {layers_policy[0][0]}.', file=sys.stderr)
            sys.exit()
        
        if layers_policy[-1][1] != action_size:
            print(f'Warning: Output of critic set to size {layers_policy[-1][1]}, setting to {action_size}.', file=sys.stderr)

        # First define the encoder neural network
        self.encoder = Encoder(obs_raw, device, layers_enc, layer_opts_enc, act_enc, out_act_enc, out_act_opts_enc)
        # Second the policy neural network - would take encoded observation as input
        self.policy_nn = Encoder(False, device, layers_policy, layer_opts_policy, act_policy, out_act_policy, out_act_opts_policy)
        # Optimizer to learn the model
        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)
        # Transfer to CUDA if possible
        # Move to device passed by parameter
        self.to(device)

    @property
    def unique_experience(self):
        '''Require authentic experience for training (experience is authentic when generated by self)'''
        # Obtain the value from config, default behaviour is to require
        return self.cnf.get('authentic_experience', True)

    def forward(self, x):
        '''
        Obtain approximated optimal policy

        @params:
            x:      Global observations
        '''
        return self.policy_nn(self.encoder(x))

    def calculate_additional_loss(self, **kwargs):
        '''
        Calculate loss apart MADDPG
        For now, it's just plain 0

        @params:
            kwargs:     Empty - would not be processed nor the program would crash
        '''
        return torch.Tensor([0])